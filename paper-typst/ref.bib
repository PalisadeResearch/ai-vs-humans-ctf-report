misc{measuring-ai-ability-to-complete-long-tasks,
    title = {Measuring AI Ability to Complete Long Tasks},
    author = {METR},
    howpublished = {\url{https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/}},
    year = {2025},
    month = {03},
    url={https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/}
}

@misc{kwa2025measuringaiabilitycomplete,
      title={Measuring AI Ability to Complete Long Tasks}, 
      author={Thomas Kwa and Ben West and Joel Becker and Amy Deng and Katharyn Garcia and Max Hasin and Sami Jawhar and Megan Kinniment and Nate Rush and Sydney Von Arx and Ryan Bloom and Thomas Broadley and Haoxing Du and Brian Goodrich and Nikola Jurkovic and Luke Harold Miles and Seraphina Nix and Tao Lin and Neev Parikh and David Rein and Lucas Jun Koba Sato and Hjalmar Wijk and Daniel M. Ziegler and Elizabeth Barnes and Lawrence Chan},
      year={2025},
      eprint={2503.14499},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2503.14499}, 
}

@misc{apollo-research-evals-gap,
  organization = {Apollo Research},
  author = {{Apollo Research}},
  howpublished = {\url{https://www.apolloresearch.ai/blog/evalsgap}},
  title = {The Evals Gap},
  year = {2025},
  month = {11},
  day = {11},
  url={https://www.apolloresearch.ai/blog/evalsgap}
}
@misc{turtayev2024hackingctfsplainagents,
      title={Hacking CTFs with Plain Agents}, 
      author={Rustem Turtayev and Artem Petrov and Dmitrii Volkov and Denis Volk},
      year={2024},
      eprint={2412.02776},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2412.02776}, 
}
@misc{projectzeroProjectNaptimeEvaluating2024,
  title = {Project {{Naptime}}: {{Evaluating Offensive Security Capabilities}} of {{Large Language Models}}},
  author = {{Project Zero}},
  year = {2024},
  month = jun,
  urldate = {2024-06-27},
  howpublished = {https://googleprojectzero.blogspot.com/2024/06/project-naptime.html}
}
@misc{bhatt2024cyberseceval2widerangingcybersecurity,
      title={CyberSecEval 2: A Wide-Ranging Cybersecurity Evaluation Suite for Large Language Models}, 
      author={Manish Bhatt and Sahana Chennabasappa and Yue Li and Cyrus Nikolaidis and Daniel Song and Shengye Wan and Faizan Ahmad and Cornelius Aschermann and Yaohui Chen and Dhaval Kapil and David Molnar and Spencer Whitman and Joshua Saxe},
      year={2024},
      eprint={2404.13161},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2404.13161}, 
}
@misc{phuongEvaluatingFrontierModels2024,
  title = {Evaluating {{Frontier Models}} for {{Dangerous Capabilities}}},
  author = {Phuong, Mary and Aitchison, Matthew and Catt, Elliot and Cogan, Sarah and Kaskasoli, Alexandre and Krakovna, Victoria and Lindner, David and Rahtz, Matthew and Assael, Yannis and Hodkinson, Sarah and Howard, Heidi and Lieberum, Tom and Kumar, Ramana and Raad, Maria Abi and Webson, Albert and Ho, Lewis and Lin, Sharon and Farquhar, Sebastian and Hutter, Marcus and Deletang, Gregoire and Ruoss, Anian and {El-Sayed}, Seliem and Brown, Sasha and Dragan, Anca and Shah, Rohin and Dafoe, Allan and Shevlane, Toby},
  year = {2024},
  month = mar,
  number = {arXiv:2403.13793},
  eprint = {2403.13793},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.13793},
  urldate = {2024-04-01},
  abstract = {To understand the risks posed by a new AI system, we must understand what it can and cannot do. Building on prior work, we introduce a programme of new "dangerous capability" evaluations and pilot them on Gemini 1.0 models. Our evaluations cover four areas: (1) persuasion and deception; (2) cyber-security; (3) self-proliferation; and (4) self-reasoning. We do not find evidence of strong dangerous capabilities in the models we evaluated, but we flag early warning signs. Our goal is to help advance a rigorous science of dangerous capability evaluation, in preparation for future models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/user/Zotero/storage/W3NEXMB5/Phuong et al. - 2024 - Evaluating Frontier Models for Dangerous Capabilit.pdf;/Users/user/Zotero/storage/EDIQZVPF/2403.html}
}

misc{yangInterCodeStandardizingBenchmarking2023,
  title = {{{InterCode}}: {{Standardizing}} and {{Benchmarking Interactive Coding}} with {{Execution Feedback}}},
  shorttitle = {{{InterCode}}},
  author = {Yang, John and Prabhakar, Akshara and Narasimhan, Karthik and Yao, Shunyu},
  year = {2023},
  month = oct,
  number = {arXiv:2306.14898},
  eprint = {2306.14898},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.14898},
  urldate = {2024-10-31},
  abstract = {Humans write code in a fundamentally interactive manner and rely on constant execution feedback to correct errors, resolve ambiguities, and decompose tasks. While LLMs have recently exhibited promising coding capabilities, current coding benchmarks mostly consider a static instruction-to-code sequence transduction process, which has the potential for error propagation and a disconnect between the generated code and its final execution environment. To address this gap, we introduce InterCode, a lightweight, flexible, and easy-to-use framework of interactive coding as a standard reinforcement learning (RL) environment, with code as actions and execution feedback as observations. Our framework is language and platform agnostic, uses self-contained Docker environments to provide safe and reproducible execution, and is compatible out-of-the-box with traditional seq2seq coding methods, while enabling the development of new methods for interactive code generation. We use InterCode to create three interactive code environments with Bash, SQL, and Python as action spaces, leveraging data from the static NL2Bash, Spider, and MBPP datasets. We demonstrate InterCode's viability as a testbed by evaluating multiple state-of-the-art LLMs configured with different prompting strategies such as ReAct and Plan \& Solve. Our results showcase the benefits of interactive code generation and demonstrate that InterCode can serve as a challenging benchmark for advancing code understanding and generation capabilities. InterCode is designed to be easily extensible and can even be used to create new tasks such as Capture the Flag, a popular coding puzzle that is inherently multi-step and involves multiple programming languages. Project site with code and data: https://intercode-benchmark.github.io},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {/Users/user/Zotero/storage/STI52M5L/Yang et al. - 2023 - InterCode Standardizing and Benchmarking Interactive Coding with Execution Feedback.pdf;/Users/user/Zotero/storage/TMFNWMM5/2306.html}
}

@misc{mayoralvilches2025caiopenbugbountyready,
      title={CAI: An Open, Bug Bounty-Ready Cybersecurity AI}, 
      author={Víctor Mayoral-Vilches and Luis Javier Navarrete-Lozano and María Sanz-Gómez and Lidia Salas Espejo and Martiño Crespo-Álvarez and Francisco Oca-Gonzalez and Francesco Balassone and Alfonso Glera-Picón and Unai Ayucar-Carbajo and Jon Ander Ruiz-Alcalde and Stefan Rass and Martin Pinzger and Endika Gil-Uriarte},
      year={2025},
      eprint={2504.06017},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2504.06017}, 
}

@inproceedings{
yang2023language,
title={Language Agents as Hackers: Evaluating Cybersecurity Skills with Capture the Flag},
author={John Yang and Akshara Prabhakar and Shunyu Yao and Kexin Pei and Karthik R Narasimhan},
booktitle={Multi-Agent Security Workshop @ NeurIPS'23},
year={2023},
url={https://openreview.net/forum?id=KOZwk7BFc3}
}